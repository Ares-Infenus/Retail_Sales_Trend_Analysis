# An谩lisis de Tendencias de Ventas Minoristas

Repositorio para el proyecto **"An谩lisis de Tendencias de Ventas Minoristas"**, centrado en las primeras fases de ingesta y exploraci贸n de datos del dataset real de ventas (Corporaci贸n Favorita Grocery Sales Forecasting de Kaggle).

---

##  Tabla de Contenidos

1. [Descripci贸n del Proyecto](#descripci贸n-del-proyecto)
2. [Progreso Actual](#progreso-actual)
3. [Escenario Empresarial](#escenario-empresarial)
4. [Estructura del Repositorio](#estructura-del-repositorio)
5. [Requisitos](#requisitos)
6. [Instalaci贸n y Configuraci贸n](#instalaci贸n-y-configuraci贸n)
7. [Flujo de Trabajo y Fases](#flujo-de-trabajo-y-fases)
8. [Integraci贸n Continua (CI/CD)](#integraci贸n-continua-cicd)
9. [Contribuir](#contribuir)
10. [Licencia](#licencia)

---

## Descripci贸n del Proyecto

Este proyecto tiene como prop贸sito realizar un an谩lisis meticuloso de los registros de ventas minoristas de Corporaci贸n Favorita (Kaggle). A trav茅s de un proceso sistem谩tico de ingesta y exploraci贸n de datos, se persigue:

- Identificar tendencias de largo plazo y patrones estacionales que influyen en la demanda.
- Detectar anomal铆as y comportamientos at铆picos que puedan revelar oportunidades o riesgos.
- Generar visualizaciones de alto impacto para interpretar indicadores clave de rendimiento (KPIs).
- Sentar las bases metodol贸gicas para la formulaci贸n de recomendaciones estrat茅gicas en gesti贸n de inventarios, programaci贸n de campa帽as promocionales y pol铆ticas de precios.

> **Visi贸n a futuro**: Desarrollar un reporte ejecutivo y un dashboard interactivo que respalden la toma de decisiones de la alta direcci贸n de una cadena retail.

Dataset base: [Corporaci贸n Favorita Grocery Sales Forecasting (Kaggle)](https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting/data)

# Flujo de Trabajo Refinado para Proyecto de An谩lisis de Ventas Minoristas

Basado en tus diagramas y arquitectura, te propongo un flujo de trabajo m谩s coherente y detallado que se alinea mejor con tu proyecto de an谩lisis de ventas minoristas.

## Definition of Done (DoD) Refinado
Cada entregable se considerar谩 completo cuando:
1. Funcionalidad verificada sin errores
2. Pasa linting (Black + Flake8)
3. Cuenta con tests cr铆ticos (pytest)
4. Incluye documentaci贸n m铆nima (docstrings o Markdown)
5. Es reproducible en Docker limpio
6. Se integra correctamente con la base de datos PostgreSQL

## D铆a 1: Setup & Infraestructura Inicial

**Entregables:**
* **Tablero Kanban** en GitHub Projects con columnas To Do, In Progress, Done y etiquetas de prioridad
* **CI Pipeline** configurado (`.github/workflows/ci.yml`) integrando Black, Flake8 y pytest
* **Docker Compose** configurado (`infra/docker-compose.yml`) con:
 - Servicio PostgreSQL
 - Vol煤menes persistentes
 - Variables de entorno en `.env` 
* **Scripts de Migraci贸n** (`infra/migrations/001_initial_schema.sql`) definiendo tablas para ventas, productos, tiendas y promociones
* **Config centralizado** (`config/config.yaml`) con par谩metros de conexi贸n y rutas
* **Test de Infraestructura** (`tests/test_db.py`) validando conexi贸n a PostgreSQL

## D铆a 2: Ingesta de Datos & EDA Inicial

**Entregables:**
* **M贸dulo de Ingesta** (`src/ingestion/ingest.py`) con:
 - Lectura chunked para datasets grandes
 - Validaci贸n de esquema inicial
 - Carga a PostgreSQL con logging
* **Notebook EDA Inicial** (`notebooks/01_EDA.ipynb`) con:
 - Conexi贸n a PostgreSQL
 - Estad铆sticas descriptivas por categor铆a y tienda
 - Visualizaciones de distribuciones y correlaciones
 - Detecci贸n de valores faltantes y outliers
* **Test de Ingesta** (`tests/test_ingest.py`) validando:
 - Correcta lectura de chunks
 - Inserci贸n en PostgreSQL
 - Manejo de tipos de datos

## D铆a 3: Limpieza & An谩lisis de Tendencias

**Entregables:**
* **M贸dulo de Limpieza** (`src/cleaning/clean.py`) con:
 - Funciones vectorizadas para imputaci贸n
 - Detecci贸n y tratamiento de outliers
 - Normalizaci贸n de categor铆as y fechas
 - Logging detallado de transformaciones
* **Notebook de Limpieza** (`notebooks/02_Cleaning.ipynb`) demostrando el flujo completo
* **Notebook de An谩lisis** (`notebooks/03_Trend_Analysis.ipynb`) con:
 - Series temporales de ventas por segmento
 - An谩lisis de estacionalidad
 - Detecci贸n de anomal铆as y eventos especiales
* **Test de Limpieza** (`tests/test_clean.py`) usando pandera para validar esquema

## D铆a 4: Modelado & Dashboard Prototipo

**Entregables:**
* **M贸dulo de Modelado** (`src/modeling/forecast.py`) con:
 - Implementaci贸n de modelos de series temporales (ARIMA/Prophet)
 - Evaluaci贸n de m茅tricas de error (MAPE, RMSE)
 - Funci贸n de predicci贸n para pr贸ximos periodos
* **Notebook de Modelado** (`notebooks/04_Modeling.ipynb`) demostrando entrenamiento y validaci贸n
* **M贸dulo de Visualizaci贸n** (`src/visualization/plots.py`) con funciones reutilizables para:
 - Gr谩ficos de tendencias
 - Comparativas de categor铆as
 - Mapas de calor por regi贸n/tienda
* **Dashboard Prototipo** (`notebooks/05_Dashboard_Prototype.ipynb`) validando componentes visuales
* **Tests de Modelado** (`tests/test_modeling.py`) verificando funciones predictivas

## D铆a 5: Dashboard Interactivo & Documentaci贸n

**Entregables:**
* **App Interactiva** (`dashboards/interactive_dashboard/app.py`) implementada en Streamlit con:
 - Filtros de rango de fechas y categor铆as
 - KPIs principales (ventas totales, variaci贸n %, top productos)
 - Visualizaci贸n geogr谩fica de tiendas
 - Predicciones de ventas futuras
* **Script de Despliegue** (`scripts/start.sh`) automatizando el arranque del entorno completo
* **Reporte Ejecutivo** (`reports/executive_report.pdf` o `docs/executive_report.md`) con:
 - Hallazgos principales del an谩lisis
 - Tendencias identificadas y anomal铆as
 - Recomendaciones estrat茅gicas basadas en datos
 - Capturas del dashboard
* **README Actualizado** con:
 - Badges de CI y cobertura de tests
 - Instrucciones de instalaci贸n con Docker
 - Enlace al dashboard desplegado
 - Estructura del proyecto y flujo de datos

## Herramientas y Est谩ndares Espec铆ficos

* **Base de Datos:** PostgreSQL en Docker con esquemas para ventas, productos, tiendas y promociones
* **An谩lisis:** pandas + NumPy para manipulaci贸n, statsmodels/Prophet para series temporales
* **Visualizaci贸n:** Matplotlib/Plotly para notebooks, Streamlit para dashboard interactivo
* **Docker:** Multi-container con PostgreSQL + servicios Python
* **Versionado de Datos:** Git-LFS para CSVs grandes (>100MB)
* **Testing:** pytest + fixtures para conexi贸n a DB de test
* **CI/CD:** GitHub Actions para tests, linting y despliegue autom谩tico
* **Documentaci贸n:** Markdown en docs/ con diagramas PlantUML renderizados

---

## Escenario Empresarial

Simulamos el rol de analista de datos en una cadena de tiendas (p.ej., Walmart). Analizamos historial de ventas (tienda, producto, fecha, promociones, ubicaci贸n) para mejorar planificaci贸n del pr贸ximo a帽o.

---

## Estructura del Repositorio

```
project-retail-sales-analysis/
 .github/                           # Workflows de CI/CD
 infra/                             # Infraestructura (Docker, migraciones SQL)
 config/                            # Configuraci贸n (YAML, .env)
 data/                              # Datos raw y processed
 docs/                              # Documentaci贸n (diagramas y esquema)
 notebooks/                         # Notebooks: EDA, limpieza, etc.
 scripts/                           # Utilidades: carga de datos, arranque
 src/                               # C贸digo fuente: ingesti贸n, limpieza, EDA, utils
 tests/                             # Pruebas unitarias e integraci贸n
 README.md                          # Este archivo
 environment.yml                    # Dependencias Conda
```

---

## Requisitos

- Docker & Docker Compose
- Python 3.8+ (Conda recomendado)
- Git

Dependencias definidas en `environment.yml`.

---

## Instalaci贸n y Configuraci贸n

1. Clonar el repositorio:
   ```bash
   git clone https://github.com/<usuario>/project-retail-sales-analysis.git
   cd project-retail-sales-analysis
   ```
2. Crear y activar entorno Conda:
   ```bash
   conda env create -f environment.yml
   conda activate retail-sales-analysis
   ```
3. Copiar variables de entorno:
   ```bash
   cp config/.env.example .env
   ```
4. Levantar PostgreSQL en Docker:
   ```bash
   docker-compose -f infra/docker-compose.yml up -d
   ```
5. Cargar datos crudos:
   ```bash
   scripts/load_data.sh
   ```

---

## Flujo de Trabajo y Fases

1. **Ingesta de datos** (en desarrollo)
2. **EDA inicial** (pendiente)
3. **Limpieza de datos** (pendiente)
4. **An谩lisis de tendencias** (pendiente)
5. **Modelado de series temporales** (opcional)
6. **Desarrollo de dashboard** (pendiente)
7. **Reporte ejecutivo** (pendiente)

Cada fase puede ejecutarse desde los notebooks o scripts correspondientes.

---

## Integraci贸n Continua (CI/CD)

Configurado para:

- Linting (flake8, black)
- Tests unitarios (pytest)

Workflow en `.github/workflows/ci.yml`.

---

## Contribuir

1. Fork del repositorio
2. Crear rama: `git checkout -b feature/nombre`
3. Commit y push
4. Abrir Pull Request

---

## Licencia

MIT License. Consulta el archivo `LICENSE` para detalles.

