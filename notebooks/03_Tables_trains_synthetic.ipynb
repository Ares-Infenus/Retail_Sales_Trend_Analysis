{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab3c6c3",
   "metadata": {},
   "source": [
    "# Ingeniería de Características y Selección Basada en Correlación\n",
    "\n",
    "En todo proceso de modelado supervisado, la calidad de las variables de entrada puede llegar a ser tan determinante como el propio algoritmo. En este notebook proponemos una estrategia estadística para segmentar nuestro espacio de características en dos grupos: uno con las **columnas más fuertemente correlacionadas** y otro con aquellas de **correlación moderada** con respecto a la variable objetivo. De este modo, podremos entrenar modelos con distintos niveles de información y comparar directamente su desempeño.\n",
    "\n",
    "Primero, ampliamos nuestro set original de datos generando nuevas variables —por ejemplo, interacciones o polinomios— que potencialmente capten patrones no lineales. A continuación, medimos la relevancia de cada característica mediante el coeficiente de correlación de Pearson:\n",
    "\n",
    "$$\n",
    "r_i = \\frac{\\mathrm{cov}(X_i, Y)}{\\sigma_{X_i}\\,\\sigma_{Y}}\n",
    "$$\n",
    "\n",
    "y cuantificamos la redundancia usando el **Factor de Inflación de la Varianza (VIF)**:\n",
    "\n",
    "$$\n",
    "\\mathrm{VIF}_i = \\frac{1}{1 - R_i^2}\n",
    "$$\n",
    "\n",
    "donde $R_i^2$ es el coeficiente de determinación al ajustar $X_i$ como variable dependiente frente al resto de columnas del dataset.\n",
    "\n",
    "Para decidir qué columnas pasan a cada grupo, estudiamos la **distribución empírica** de los valores $|r_i|$. Definimos dos umbrales basados en percentiles:\n",
    "\n",
    "- El **66.6 %** señala el corte para nuestra **Tabla “Relevante”** (correlaciones más altas).  \n",
    "- El **33.3 %** marca el límite mínimo para la **Tabla “Semi-Relevante”** (correlaciones moderadas).\n",
    "\n",
    "El resto de las variables, con $|r_i|$ por debajo del percentil 33.3, queda fuera del conjunto de entrenamiento por aportar muy poca señal.\n",
    "\n",
    "En la siguiente sección presentamos una visualización con una curva normal de referencia:\n",
    "\n",
    "![Distribución de correlaciones con la variable objetivo](Graphics\\Ej_filtrado_correlacion.png)\n",
    "\n",
    "Las áreas sombreadas a la derecha de cada umbral permiten apreciar de un solo vistazo la proporción de columnas seleccionadas.\n",
    "\n",
    "Finalmente, crearemos dos DataFrames —uno con las columnas del grupo “Relevante” y otro con las del grupo “Semi-Relevante”— y aplicaremos la **ecuación de pronóstico** proporcionada por el concurso para predecir el número de unidades vendidas en los 16 días siguientes. Evaluaremos cada modelo comparando métricas de error de regresión como **MAE** (Error Absoluto Medio) y **RMSE** (Raíz del Error Cuadrático Medio) para determinar si el uso de un conjunto de variables más amplio (Semi-Relevante) mejora o empeora las predicciones frente al conjunto más restringido (Relevante). Esta metodología aporta rigor estadístico y flexibilidad para explorar el trade-off entre complejidad del modelo y precisión de la predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89648a4",
   "metadata": {},
   "source": [
    "Claro, aquí tienes una versión mejorada, más clara, coherente y profesional del texto, junto con la fórmula de variación porcentual diaria y un ejemplo más estructurado del análisis de desfase:\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a utilizar como plantilla base la tabla `train`, a partir de la cual crearemos dos nuevas copias denominadas `Test_50_insight` y `Test_100_insight`.\n",
    "\n",
    "Planeo trabajar con el DataFrame `oil`, del cual extraeremos las columnas `date` y `dcoilwtico` (precio del petróleo WTI). Con esta información, calcularemos la **variación porcentual diaria del precio del petróleo** y la almacenaremos en una nueva columna llamada `oil_price`.\n",
    "\n",
    "Esta columna será posteriormente combinada con la tabla `train`, alineando los valores por la columna `date`, para así conocer cómo varió el precio del petróleo cada día específico.\n",
    "\n",
    "### Cálculo de la variación porcentual diaria\n",
    "\n",
    "La fórmula para calcular la variación porcentual diaria es:\n",
    "\n",
    "$$\n",
    "\\text{variación}_{t} = \\frac{P_{t} - P_{t-1}}{P_{t-1}} \\times 100\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $P_t$ es el precio del petróleo en el día actual\n",
    "* $P_{t-1}$ es el precio del día anterior\n",
    "\n",
    "### Justificación del desfase temporal\n",
    "\n",
    "Inicialmente se pensó en **adelantar un día** la variable `oil_price`, ya que el precio del petróleo actual podría influir en las decisiones del mercado **al día siguiente**, no en el mismo día en que se registra el precio. Sin embargo, para tomar una decisión informada, se propone realizar un análisis de **correlación** entre la variable `oil_price` y otras variables objetivo, considerando tres escenarios distintos:\n",
    "\n",
    "1. **Desfase -1 (precio del día anterior):**\n",
    "   Se utiliza el precio del petróleo del día anterior para predecir el comportamiento del día actual.\n",
    "2. **Sin desfase (precio actual):**\n",
    "   Se utiliza el precio del petróleo del mismo día.\n",
    "3. **Desfase +1 (precio del día siguiente):**\n",
    "   Se usa el precio del día siguiente, bajo la hipótesis de que el precio de hoy refleja la reacción al comportamiento del día anterior.\n",
    "\n",
    "### Ejemplo ilustrativo de desfases\n",
    "\n",
    "Aquí tienes un ejemplo más claro de cómo funciona el desfase en los precios del petróleo. Supongamos que los precios diarios son los siguientes:\n",
    "\n",
    "| Fecha      | Precio WTI |\n",
    "| ---------- | ---------- |\n",
    "| 17 de mayo | 1.01       |\n",
    "| 18 de mayo | 1.05       |\n",
    "| 19 de mayo | 1.02       |\n",
    "\n",
    "Y que nuestra plantilla `train` contiene las mismas fechas como índice para unir:\n",
    "\n",
    "| Fecha      | … otras columnas … |\n",
    "| ---------- | ------------------ |\n",
    "| 17 de mayo | …                  |\n",
    "| 18 de mayo | …                  |\n",
    "| 19 de mayo | …                  |\n",
    "\n",
    "Ahora veamos los tres escenarios:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Desfase -1 (precio del día anterior)\n",
    "\n",
    "Aquí asignamos al 18 de mayo el precio del 17 de mayo, y al 19 de mayo el del 18 de mayo:\n",
    "\n",
    "| Fecha (train) | oil\\_price (t-1) |\n",
    "| ------------- | ---------------- |\n",
    "| 17 de mayo    | —                |\n",
    "| 18 de mayo    | 1.01             |\n",
    "| 19 de mayo    | 1.05             |\n",
    "\n",
    "* **Interpretación**: Para el 18 usas el precio del 17, asumiendo que el mercado reacciona con un día de retraso.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Sin desfase (precio del mismo día)\n",
    "\n",
    "Asignamos a cada fecha su propio precio:\n",
    "\n",
    "| Fecha (train) | oil\\_price (t) |\n",
    "| ------------- | -------------- |\n",
    "| 17 de mayo    | 1.01           |\n",
    "| 18 de mayo    | 1.05           |\n",
    "| 19 de mayo    | 1.02           |\n",
    "\n",
    "* **Interpretación**: El modelo ve el precio del petróleo del mismo día.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Desfase +1 (precio del día siguiente)\n",
    "\n",
    "Aquí adelantamos un día: al 17 de mayo le ponemos el precio del 18, y al 18 el del 19:\n",
    "\n",
    "| Fecha (train) | oil\\_price (t+1) |\n",
    "| ------------- | ---------------- |\n",
    "| 17 de mayo    | 1.05             |\n",
    "| 18 de mayo    | 1.02             |\n",
    "| 19 de mayo    | —                |\n",
    "\n",
    "* **Interpretación**: El modelo “ve” el precio que estará disponible mañana, ideal si crees que las señales del petróleo ya anticipan movimientos un día antes.\n",
    "\n",
    "---\n",
    "\n",
    "Con estos tres conjuntos de datos podrás calcular el coeficiente de correlación de cada versión de `oil_price` con tu variable objetivo y decidir cuál desfase aporta más valor predictivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82244ca8",
   "metadata": {},
   "source": [
    "## `HOLIDAYS_Events`\n",
    "\n",
    "Este es otro *dataframe* que contiene columnas con información útil que podemos aprovechar.  \n",
    "La idea es crear dos nuevas columnas en nuestro conjunto de datos `train` (y también `test`):\n",
    "\n",
    "1. **`es_festivo`**: una columna booleana que indicará si la fecha corresponde a un día festivo (`True` o `False`).\n",
    "2. **`tipo_festivo`**: una columna categórica que especificará el tipo de día festivo. Para facilitar su uso en modelos de IA (que no aceptan variables de tipo `string` directamente), se codificará de forma numérica:\n",
    "\n",
    "   - `0`: festivo local  \n",
    "   - `1`: festivo regional  \n",
    "   - `2`: festivo nacional\n",
    "\n",
    "> ⚠️ **Nota importante:**  \n",
    "Para clasificar correctamente si un día es festivo y su tipo, se debe tener en cuenta la información contenida en `stores.csv`.  \n",
    "Cada fila de los dataframes `train` y `test` contiene una columna llamada `store_nbr`, que representa el número de tienda.  \n",
    "Mediante este número, se puede localizar la tienda correspondiente en `stores.csv`, que contiene datos clave como la ciudad, estado o región de cada tienda.  \n",
    "Posteriormente, al cruzar esa ubicación con la fecha correspondiente en el dataframe `HOLIDAYS_Events`, es posible determinar:\n",
    "\n",
    "- Si ese día es festivo o no para esa tienda (`es_festivo`)\n",
    "- Y en caso afirmativo, el tipo de festivo (`tipo_festivo`: local, regional o nacional)\n",
    "\n",
    "Estas nuevas columnas nos permitirán enriquecer el análisis y posiblemente mejorar el rendimiento del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37e3e5",
   "metadata": {},
   "source": [
    "Del DataFrame `items` extraeremos las columnas `family`, `class` y `perishable` con el objetivo de convertirlas a variables numéricas. En particular, la columna `perishable` se transformará en una variable binaria, donde:\n",
    "\n",
    "- `1` indicará que el producto es perecedero,\n",
    "- `0` que no lo es.\n",
    "\n",
    "Por otro lado, del DataFrame `transactions` extraeremos la columna `transactions`, ya que la utilizaremos como variable explicativa en el análisis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb37694",
   "metadata": {},
   "source": [
    "## Estructura Final del DataFrame Integrado\n",
    "\n",
    "A continuación se muestra un ejemplo de cómo quedaría la arquitectura final del DataFrame después de integrar y transformar las distintas fuentes de datos (`sales`, `items`, `transactions`, `holidays_events`, `oil`), dejando todas las columnas en formato numérico:\n",
    "\n",
    "| id  | date       | store_nbr | item_nbr | unit_sales | onpromotion | is_festive | type_festive | type_local_festive | Price_oil_pct | family_items | class_items | perishable | transactions |\n",
    "|-----|------------|-----------|----------|------------|-------------|------------|--------------|---------------------|----------------|---------------|-------------|------------|--------------|\n",
    "| 1   | 2017-08-15 | 1         | 103520   | 3.0        | 1           | 1          | 2            | 1                   | 0.012          | 4             | 1013        | 1          | 1345         |\n",
    "| 2   | 2017-08-15 | 1         | 105574   | 0.0        | 0           | 1          | 2            | 1                   | 0.012          | 2             | 2020        | 1          | 1345         |\n",
    "| 3   | 2017-08-16 | 2         | 103520   | 5.0        | 1           | 0          | 0            | 0                   | -0.006         | 4             | 1013        | 1          | 1120         |\n",
    "| 4   | 2017-08-16 | 2         | 209211   | 8.0        | 0           | 0          | 0            | 0                   | -0.006         | 3             | 3001        | 1          | 1120         |\n",
    "\n",
    "### Descripción de columnas\n",
    "\n",
    "- **`store_nbr`**: Identificador numérico de la tienda.\n",
    "- **`item_nbr`**: Identificador numérico del producto.\n",
    "- **`unit_sales`**: Unidades vendidas del producto en esa fecha y tienda.\n",
    "- **`onpromotion`**: Indicador binario (`1` si el producto estaba en promoción, `0` si no).\n",
    "- **`is_festive`**: Indicador binario (`1` si la fecha es festiva, `0` si no).\n",
    "- **`type_festive`**: Tipo de festividad codificado numéricamente (ej. `0`: Ninguna, `1`: Evento, `2`: Holiday).\n",
    "- **`type_local_festive`**: Indicador binario (`1` si la festividad es local, `0` si no).\n",
    "- **`Price_oil_pct`**: Variación porcentual del precio del petróleo respecto al día anterior.\n",
    "- **`family_items`**: Categoría general del producto codificada numéricamente (Label Encoding).\n",
    "- **`class_items`**: Subcategoría o clase numérica del producto.\n",
    "- **`perishable`**: Indicador binario (`1` si el producto es perecedero, `0` si no).\n",
    "- **`transactions`**: Número total de transacciones realizadas en la tienda ese día.\n",
    "\n",
    "Este DataFrame está completamente listo para ser utilizado en modelos de predicción o análisis multivariado, cumpliendo con el requisito de tener únicamente variables numéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8520dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\transactions.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\transactions\n",
      "Particiones: 1\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\transactions\n",
      "\n",
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\test.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\test\n",
      "Particiones: 1\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\test\n",
      "\n",
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\stores.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\stores\n",
      "Particiones: 1\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\stores\n",
      "\n",
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\items.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\items\n",
      "Particiones: 1\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\items\n",
      "\n",
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\holidays_events.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\holidays_events\n",
      "Particiones: 1\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\holidays_events\n",
      "\n",
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\clear_train.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\clear_train\n",
      "Particiones: 89\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\clear_train\n",
      "\n",
      "Procesando D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\clear_oil_raw_Dukascopy.csv -> D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\clear_oil_raw_Dukascopy\n",
      "Particiones: 1\n",
      "✅ Guardado en: D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\clear_oil_raw_Dukascopy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pathlib\n",
    "\n",
    "# 2. Define los archivos CSV de entrada y la carpeta de salida base\n",
    "input_files = [\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\transactions.csv\",\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\test.csv\",\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\stores.csv\",\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\items.csv\",\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\holidays_events.csv\",\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\clear_train.csv\",\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\clear_oil_raw_Dukascopy.csv\",\n",
    "]\n",
    "\n",
    "base_output_dir = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\"\n",
    "chunk_size_mb = 64\n",
    "blocksize = chunk_size_mb * 1024 ** 2\n",
    "\n",
    "# 3. Procesa cada archivo CSV individualmente\n",
    "def convert_csv_to_parquet(csv_path):\n",
    "    file_name = pathlib.Path(csv_path).stem\n",
    "    output_path = os.path.join(base_output_dir, file_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Procesando {csv_path} -> {output_path}\")\n",
    "\n",
    "    df = dd.read_csv(\n",
    "        csv_path,\n",
    "        blocksize=blocksize,\n",
    "        assume_missing=True\n",
    "    )\n",
    "\n",
    "    print(f\"Particiones: {df.npartitions}\")\n",
    "\n",
    "    df.to_parquet(\n",
    "        output_path,\n",
    "        write_index=False,\n",
    "        compression='snappy',\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Guardado en: {output_path}\\n\")\n",
    "\n",
    "for csv_file in input_files:\n",
    "    convert_csv_to_parquet(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# ----------------------------\n",
    "# Rutas de archivos\n",
    "# ----------------------------\n",
    "oil_path           = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\clear_oil_raw_Dukascopy\"\n",
    "transactions_path  = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\transactions\"\n",
    "items_path         = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\items\"\n",
    "stores_path        = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\stores\"\n",
    "clear_train_path   = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\clear_train\"\n",
    "holidays_path      = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\holidays_events\"\n",
    "output_path        = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\cleaned_ready\\train_with_oil_tx_items_stores_festive.parquet\"\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Leer y procesar datos de Oil\n",
    "# ----------------------------\n",
    "oil_dd = dd.read_parquet(oil_path, engine=\"pyarrow\")\n",
    "oil_dd = oil_dd.assign(prev_price=oil_dd[\"dcoilwtico\"].shift(1))\n",
    "oil_dd = (\n",
    "    oil_dd\n",
    "    .assign(\n",
    "        Price_oil_pct=((oil_dd[\"dcoilwtico\"] - oil_dd[\"prev_price\"]) / oil_dd[\"prev_price\"]) * 100\n",
    "    )\n",
    "    .fillna({\"Price_oil_pct\": 0})\n",
    "    .loc[:, [\"date\", \"Price_oil_pct\"]]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Leer transacciones\n",
    "# ----------------------------\n",
    "tx_dd = (\n",
    "    dd.read_parquet(transactions_path, engine=\"pyarrow\")\n",
    "      .astype({\"store_nbr\": \"int32\"})\n",
    "      [[\"date\", \"store_nbr\", \"transactions\"]]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Leer información de items\n",
    "# ----------------------------\n",
    "items_dd = (\n",
    "    dd.read_parquet(items_path, engine=\"pyarrow\")\n",
    "      [[\"item_nbr\", \"family\", \"class\", \"perishable\"]]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Leer train base\n",
    "# ----------------------------\n",
    "train_dd = (\n",
    "    dd.read_parquet(clear_train_path, engine=\"pyarrow\")\n",
    "      .astype({\"store_nbr\": \"int32\"})\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Merge Oil\n",
    "# ----------------------------\n",
    "train_dd = train_dd.merge(oil_dd, on=\"date\", how=\"left\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Merge Transacciones y rellenar nulos\n",
    "# ----------------------------\n",
    "train_dd = train_dd.merge(\n",
    "    tx_dd,\n",
    "    on=[\"date\", \"store_nbr\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "train_dd = train_dd.assign(\n",
    "    transactions = train_dd[\"transactions\"].fillna(0).astype(\"int32\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Merge Ítems\n",
    "# ----------------------------\n",
    "train_dd = train_dd.merge(items_dd, on=\"item_nbr\", how=\"left\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Merge Stores (ciudad y estado)\n",
    "# ----------------------------\n",
    "stores_dd = (\n",
    "    dd.read_parquet(stores_path, engine=\"pyarrow\")\n",
    "      .astype({\"store_nbr\": \"int32\"})\n",
    "      [[\"store_nbr\", \"city\", \"state\"]]\n",
    ")\n",
    "train_dd = train_dd.merge(\n",
    "    stores_dd,\n",
    "    on=\"store_nbr\",\n",
    "    how=\"left\"\n",
    ")\n",
    "train_dd = train_dd.assign(\n",
    "    city  = train_dd[\"city\"].fillna(\"Unknown\"),\n",
    "    state = train_dd[\"state\"].fillna(\"Unknown\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Leer holidays_events (incluimos locale_name)\n",
    "# ----------------------------\n",
    "hol_dd = (\n",
    "    dd.read_parquet(holidays_path, engine=\"pyarrow\")\n",
    "      [[\"date\", \"type\", \"locale\", \"locale_name\", \"transferred\"]]\n",
    "      .rename(columns={\n",
    "          \"type\": \"holiday_type\",\n",
    "          \"locale\": \"holiday_locale\",\n",
    "          \"locale_name\": \"holiday_locale_name\"\n",
    "      })\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10) Merge Festivos y crear los flags con la nueva lógica\n",
    "# ----------------------------\n",
    "train_dd = train_dd.merge(\n",
    "    hol_dd,\n",
    "    on=\"date\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Normalizamos `transferred` a boolean\n",
    "train_dd = train_dd.assign(\n",
    "    transferred = train_dd[\"transferred\"].fillna(False)\n",
    ")\n",
    "\n",
    "# definimos el flag is_festive combinando las tres reglas\n",
    "train_dd = train_dd.assign(\n",
    "    is_festive=(\n",
    "        ((train_dd[\"holiday_locale\"] == \"National\") & (~train_dd[\"transferred\"])) |\n",
    "        ((train_dd[\"holiday_locale\"] == \"Regional\") & (train_dd[\"state\"] == train_dd[\"holiday_locale_name\"])) |\n",
    "        ((train_dd[\"holiday_locale\"] == \"Local\") &\n",
    "         (train_dd[\"city\"] == train_dd[\"holiday_locale_name\"]) &\n",
    "         (~train_dd[\"transferred\"]))\n",
    "    )\n",
    ")\n",
    "\n",
    "# asignamos los valores de tipo y nivel solo donde is_festive==True\n",
    "train_dd = train_dd.assign(\n",
    "    type_festive       = train_dd[\"holiday_type\"].where(train_dd[\"is_festive\"], other=None),\n",
    "    local_type_festive = train_dd[\"holiday_locale\"].where(train_dd[\"is_festive\"], other=None)\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10.b) Rellenar valores por defecto y limpiar tipos\n",
    "# ----------------------------\n",
    "# Usamos map_partitions para aplicar fillna + infer_objects en cada partición\n",
    "import pandas as pd\n",
    "\n",
    "train_dd = train_dd.map_partitions(\n",
    "    lambda df: (\n",
    "        df\n",
    "        .assign(\n",
    "            is_festive=lambda d: d[\"is_festive\"].fillna(False),\n",
    "            type_festive=lambda d: d[\"type_festive\"].fillna(\"Normal Day\"),\n",
    "            local_type_festive=lambda d: d[\"local_type_festive\"].fillna(\"Sin especificar\")\n",
    "        )\n",
    "        .infer_objects(copy=False)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10.c) Eliminar las columnas que ya no necesitamos\n",
    "# ----------------------------\n",
    "train_dd = train_dd.drop(columns=[\n",
    "    \"holiday_type\",\n",
    "    \"holiday_locale\",\n",
    "    \"holiday_locale_name\",\n",
    "])\n",
    "\n",
    "bool_cols = [col for col, dtype in train_dd.dtypes.items() if dtype == 'bool']\n",
    "train_dd = train_dd.astype({c: 'int8' for c in bool_cols})\n",
    "\n",
    "# ----------------------------\n",
    "# 11) Guardar DataFrame final\n",
    "# ----------------------------\n",
    "train_dd.to_parquet(\n",
    "    output_path,\n",
    "    engine=\"pyarrow\",\n",
    "    write_index=False\n",
    ")\n",
    "# ----------------------------\n",
    "# 12) Label Encoding de categóricas\n",
    "# ----------------------------\n",
    "cat_cols = [\n",
    "    \"family\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"type_festive\",\n",
    "    \"local_type_festive\"\n",
    "]\n",
    "\n",
    "# 1) Aseguramos categorías «conocidas» en Dask\n",
    "train_dd = train_dd.categorize(columns=cat_cols)\n",
    "\n",
    "# 2) Extraemos los códigos\n",
    "for col in cat_cols:\n",
    "    train_dd[col + \"_enc\"] = train_dd[col].cat.codes\n",
    "\n",
    "# 3) --- NEW: Construimos y guardamos el diccionario de categorías ANTES de dropear ---\n",
    "import json\n",
    "categories_map = {}\n",
    "for col in cat_cols:\n",
    "    # `train_dd[col]` todavía existe, así que podemos usar `.cat.categories`\n",
    "    categories_map[col] = train_dd[col].cat.categories.tolist()\n",
    "\n",
    "with open(\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\logs\\mapeado_train\\categories_map.json\",\n",
    "    \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump(categories_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 4) Ahora sí, eliminamos las columnas originales\n",
    "train_dd = train_dd.drop(columns=cat_cols)\n",
    "s\n",
    "# ----------------------------\n",
    "# 13) Guardar DataFrame final con codificaciones\n",
    "# ----------------------------\n",
    "train_dd.to_parquet(\n",
    "    output_path,\n",
    "    engine=\"pyarrow\",\n",
    "    write_index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696701b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
