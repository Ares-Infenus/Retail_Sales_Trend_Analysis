{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab3c6c3",
   "metadata": {},
   "source": [
    "# Ingeniería de Características y Selección Basada en Correlación\n",
    "\n",
    "En todo proceso de modelado supervisado, la calidad de las variables de entrada puede llegar a ser tan determinante como el propio algoritmo. En este notebook proponemos una estrategia estadística para segmentar nuestro espacio de características en dos grupos: uno con las **columnas más fuertemente correlacionadas** y otro con aquellas de **correlación moderada** con respecto a la variable objetivo. De este modo, podremos entrenar modelos con distintos niveles de información y comparar directamente su desempeño.\n",
    "\n",
    "Primero, ampliamos nuestro set original de datos generando nuevas variables —por ejemplo, interacciones o polinomios— que potencialmente capten patrones no lineales. A continuación, medimos la relevancia de cada característica mediante el coeficiente de correlación de Pearson:\n",
    "\n",
    "$$\n",
    "r_i = \\frac{\\mathrm{cov}(X_i, Y)}{\\sigma_{X_i}\\,\\sigma_{Y}}\n",
    "$$\n",
    "\n",
    "y cuantificamos la redundancia usando el **Factor de Inflación de la Varianza (VIF)**:\n",
    "\n",
    "$$\n",
    "\\mathrm{VIF}_i = \\frac{1}{1 - R_i^2}\n",
    "$$\n",
    "\n",
    "donde $R_i^2$ es el coeficiente de determinación al ajustar $X_i$ como variable dependiente frente al resto de columnas del dataset.\n",
    "\n",
    "Para decidir qué columnas pasan a cada grupo, estudiamos la **distribución empírica** de los valores $|r_i|$. Definimos dos umbrales basados en percentiles:\n",
    "\n",
    "- El **66.6 %** señala el corte para nuestra **Tabla “Relevante”** (correlaciones más altas).  \n",
    "- El **33.3 %** marca el límite mínimo para la **Tabla “Semi-Relevante”** (correlaciones moderadas).\n",
    "\n",
    "El resto de las variables, con $|r_i|$ por debajo del percentil 33.3, queda fuera del conjunto de entrenamiento por aportar muy poca señal.\n",
    "\n",
    "En la siguiente sección presentamos una visualización con una curva normal de referencia:\n",
    "\n",
    "![Distribución de correlaciones con la variable objetivo](Graphics\\Ej_filtrado_correlacion.png)\n",
    "\n",
    "Las áreas sombreadas a la derecha de cada umbral permiten apreciar de un solo vistazo la proporción de columnas seleccionadas.\n",
    "\n",
    "Finalmente, crearemos dos DataFrames —uno con las columnas del grupo “Relevante” y otro con las del grupo “Semi-Relevante”— y aplicaremos la **ecuación de pronóstico** proporcionada por el concurso para predecir el número de unidades vendidas en los 16 días siguientes. Evaluaremos cada modelo comparando métricas de error de regresión como **MAE** (Error Absoluto Medio) y **RMSE** (Raíz del Error Cuadrático Medio) para determinar si el uso de un conjunto de variables más amplio (Semi-Relevante) mejora o empeora las predicciones frente al conjunto más restringido (Relevante). Esta metodología aporta rigor estadístico y flexibilidad para explorar el trade-off entre complejidad del modelo y precisión de la predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89648a4",
   "metadata": {},
   "source": [
    "Claro, aquí tienes una versión mejorada, más clara, coherente y profesional del texto, junto con la fórmula de variación porcentual diaria y un ejemplo más estructurado del análisis de desfase:\n",
    "\n",
    "---\n",
    "\n",
    "Vamos a utilizar como plantilla base la tabla `train`, a partir de la cual crearemos dos nuevas copias denominadas `Test_50_insight` y `Test_100_insight`.\n",
    "\n",
    "Planeo trabajar con el DataFrame `oil`, del cual extraeremos las columnas `date` y `dcoilwtico` (precio del petróleo WTI). Con esta información, calcularemos la **variación porcentual diaria del precio del petróleo** y la almacenaremos en una nueva columna llamada `oil_price`.\n",
    "\n",
    "Esta columna será posteriormente combinada con la tabla `train`, alineando los valores por la columna `date`, para así conocer cómo varió el precio del petróleo cada día específico.\n",
    "\n",
    "### Cálculo de la variación porcentual diaria\n",
    "\n",
    "La fórmula para calcular la variación porcentual diaria es:\n",
    "\n",
    "$$\n",
    "\\text{variación}_{t} = \\frac{P_{t} - P_{t-1}}{P_{t-1}} \\times 100\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $P_t$ es el precio del petróleo en el día actual\n",
    "* $P_{t-1}$ es el precio del día anterior\n",
    "\n",
    "### Justificación del desfase temporal\n",
    "\n",
    "Inicialmente se pensó en **adelantar un día** la variable `oil_price`, ya que el precio del petróleo actual podría influir en las decisiones del mercado **al día siguiente**, no en el mismo día en que se registra el precio. Sin embargo, para tomar una decisión informada, se propone realizar un análisis de **correlación** entre la variable `oil_price` y otras variables objetivo, considerando tres escenarios distintos:\n",
    "\n",
    "1. **Desfase -1 (precio del día anterior):**\n",
    "   Se utiliza el precio del petróleo del día anterior para predecir el comportamiento del día actual.\n",
    "2. **Sin desfase (precio actual):**\n",
    "   Se utiliza el precio del petróleo del mismo día.\n",
    "3. **Desfase +1 (precio del día siguiente):**\n",
    "   Se usa el precio del día siguiente, bajo la hipótesis de que el precio de hoy refleja la reacción al comportamiento del día anterior.\n",
    "\n",
    "### Ejemplo ilustrativo de desfases\n",
    "\n",
    "Aquí tienes un ejemplo más claro de cómo funciona el desfase en los precios del petróleo. Supongamos que los precios diarios son los siguientes:\n",
    "\n",
    "| Fecha      | Precio WTI |\n",
    "| ---------- | ---------- |\n",
    "| 17 de mayo | 1.01       |\n",
    "| 18 de mayo | 1.05       |\n",
    "| 19 de mayo | 1.02       |\n",
    "\n",
    "Y que nuestra plantilla `train` contiene las mismas fechas como índice para unir:\n",
    "\n",
    "| Fecha      | … otras columnas … |\n",
    "| ---------- | ------------------ |\n",
    "| 17 de mayo | …                  |\n",
    "| 18 de mayo | …                  |\n",
    "| 19 de mayo | …                  |\n",
    "\n",
    "Ahora veamos los tres escenarios:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Desfase -1 (precio del día anterior)\n",
    "\n",
    "Aquí asignamos al 18 de mayo el precio del 17 de mayo, y al 19 de mayo el del 18 de mayo:\n",
    "\n",
    "| Fecha (train) | oil\\_price (t-1) |\n",
    "| ------------- | ---------------- |\n",
    "| 17 de mayo    | —                |\n",
    "| 18 de mayo    | 1.01             |\n",
    "| 19 de mayo    | 1.05             |\n",
    "\n",
    "* **Interpretación**: Para el 18 usas el precio del 17, asumiendo que el mercado reacciona con un día de retraso.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Sin desfase (precio del mismo día)\n",
    "\n",
    "Asignamos a cada fecha su propio precio:\n",
    "\n",
    "| Fecha (train) | oil\\_price (t) |\n",
    "| ------------- | -------------- |\n",
    "| 17 de mayo    | 1.01           |\n",
    "| 18 de mayo    | 1.05           |\n",
    "| 19 de mayo    | 1.02           |\n",
    "\n",
    "* **Interpretación**: El modelo ve el precio del petróleo del mismo día.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Desfase +1 (precio del día siguiente)\n",
    "\n",
    "Aquí adelantamos un día: al 17 de mayo le ponemos el precio del 18, y al 18 el del 19:\n",
    "\n",
    "| Fecha (train) | oil\\_price (t+1) |\n",
    "| ------------- | ---------------- |\n",
    "| 17 de mayo    | 1.05             |\n",
    "| 18 de mayo    | 1.02             |\n",
    "| 19 de mayo    | —                |\n",
    "\n",
    "* **Interpretación**: El modelo “ve” el precio que estará disponible mañana, ideal si crees que las señales del petróleo ya anticipan movimientos un día antes.\n",
    "\n",
    "---\n",
    "\n",
    "Con estos tres conjuntos de datos podrás calcular el coeficiente de correlación de cada versión de `oil_price` con tu variable objetivo y decidir cuál desfase aporta más valor predictivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82244ca8",
   "metadata": {},
   "source": [
    "## `HOLIDAYS_Events`\n",
    "\n",
    "Este es otro *dataframe* que contiene columnas con información útil que podemos aprovechar.  \n",
    "La idea es crear dos nuevas columnas en nuestro conjunto de datos `train` (y también `test`):\n",
    "\n",
    "1. **`es_festivo`**: una columna booleana que indicará si la fecha corresponde a un día festivo (`True` o `False`).\n",
    "2. **`tipo_festivo`**: una columna categórica que especificará el tipo de día festivo. Para facilitar su uso en modelos de IA (que no aceptan variables de tipo `string` directamente), se codificará de forma numérica:\n",
    "\n",
    "   - `0`: festivo local  \n",
    "   - `1`: festivo regional  \n",
    "   - `2`: festivo nacional\n",
    "\n",
    "> ⚠️ **Nota importante:**  \n",
    "Para clasificar correctamente si un día es festivo y su tipo, se debe tener en cuenta la información contenida en `stores.csv`.  \n",
    "Cada fila de los dataframes `train` y `test` contiene una columna llamada `store_nbr`, que representa el número de tienda.  \n",
    "Mediante este número, se puede localizar la tienda correspondiente en `stores.csv`, que contiene datos clave como la ciudad, estado o región de cada tienda.  \n",
    "Posteriormente, al cruzar esa ubicación con la fecha correspondiente en el dataframe `HOLIDAYS_Events`, es posible determinar:\n",
    "\n",
    "- Si ese día es festivo o no para esa tienda (`es_festivo`)\n",
    "- Y en caso afirmativo, el tipo de festivo (`tipo_festivo`: local, regional o nacional)\n",
    "\n",
    "Estas nuevas columnas nos permitirán enriquecer el análisis y posiblemente mejorar el rendimiento del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37e3e5",
   "metadata": {},
   "source": [
    "Del DataFrame `items` extraeremos las columnas `family`, `class` y `perishable` con el objetivo de convertirlas a variables numéricas. En particular, la columna `perishable` se transformará en una variable binaria, donde:\n",
    "\n",
    "- `1` indicará que el producto es perecedero,\n",
    "- `0` que no lo es.\n",
    "\n",
    "Por otro lado, del DataFrame `transactions` extraeremos la columna `transactions`, ya que la utilizaremos como variable explicativa en el análisis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb37694",
   "metadata": {},
   "source": [
    "## Estructura Final del DataFrame Integrado\n",
    "\n",
    "A continuación se muestra un ejemplo de cómo quedaría la arquitectura final del DataFrame después de integrar y transformar las distintas fuentes de datos (`sales`, `items`, `transactions`, `holidays_events`, `oil`), dejando todas las columnas en formato numérico:\n",
    "\n",
    "| id  | date       | store_nbr | item_nbr | unit_sales | onpromotion | is_festive | type_festive | type_local_festive | Price_oil_pct | family_items | class_items | perishable | transactions |\n",
    "|-----|------------|-----------|----------|------------|-------------|------------|--------------|---------------------|----------------|---------------|-------------|------------|--------------|\n",
    "| 1   | 2017-08-15 | 1         | 103520   | 3.0        | 1           | 1          | 2            | 1                   | 0.012          | 4             | 1013        | 1          | 1345         |\n",
    "| 2   | 2017-08-15 | 1         | 105574   | 0.0        | 0           | 1          | 2            | 1                   | 0.012          | 2             | 2020        | 1          | 1345         |\n",
    "| 3   | 2017-08-16 | 2         | 103520   | 5.0        | 1           | 0          | 0            | 0                   | -0.006         | 4             | 1013        | 1          | 1120         |\n",
    "| 4   | 2017-08-16 | 2         | 209211   | 8.0        | 0           | 0          | 0            | 0                   | -0.006         | 3             | 3001        | 1          | 1120         |\n",
    "\n",
    "### Descripción de columnas\n",
    "\n",
    "- **`store_nbr`**: Identificador numérico de la tienda.\n",
    "- **`item_nbr`**: Identificador numérico del producto.\n",
    "- **`unit_sales`**: Unidades vendidas del producto en esa fecha y tienda.\n",
    "- **`onpromotion`**: Indicador binario (`1` si el producto estaba en promoción, `0` si no).\n",
    "- **`is_festive`**: Indicador binario (`1` si la fecha es festiva, `0` si no).\n",
    "- **`type_festive`**: Tipo de festividad codificado numéricamente (ej. `0`: Ninguna, `1`: Evento, `2`: Holiday).\n",
    "- **`type_local_festive`**: Indicador binario (`1` si la festividad es local, `0` si no).\n",
    "- **`Price_oil_pct`**: Variación porcentual del precio del petróleo respecto al día anterior.\n",
    "- **`family_items`**: Categoría general del producto codificada numéricamente (Label Encoding).\n",
    "- **`class_items`**: Subcategoría o clase numérica del producto.\n",
    "- **`perishable`**: Indicador binario (`1` si el producto es perecedero, `0` si no).\n",
    "- **`transactions`**: Número total de transacciones realizadas en la tienda ese día.\n",
    "\n",
    "Este DataFrame está completamente listo para ser utilizado en modelos de predicción o análisis multivariado, cumpliendo con el requisito de tener únicamente variables numéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ca8d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando chunks: 419chunk [14:30,  2.08s/chunk]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Cargo y preparo el df pequeño de precios de crudo\n",
    "oil_df = pd.read_csv(\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\clear_oil_raw_Dukascopy.csv\",\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "oil_df = oil_df.sort_values(\"date\")\n",
    "oil_df[\"pct_change\"] = oil_df[\"dcoilwtico\"].pct_change() * 100\n",
    "oil_df[\"pct_change\"] = oil_df[\"pct_change\"].fillna(0)\n",
    "oil_df = oil_df[[\"date\", \"pct_change\"]].rename(columns={\"pct_change\": \"Price_oil_pct\"})\n",
    "\n",
    "# 1b) Cargo el dataframe de transacciones\n",
    "transactions_df = pd.read_csv(\n",
    "    r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\raw\\transactions.csv\",\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "# Asegúrate de que store_nbr tenga el mismo tipo (int o float) en ambos dfs\n",
    "transactions_df[\"store_nbr\"] = transactions_df[\"store_nbr\"].astype(oil_df.dtypes.get(\"store_nbr\", \"int64\"))\n",
    "\n",
    "# 2) Configuración de paths y tamaño de chunk\n",
    "input_path   = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\clear_train.csv\"\n",
    "output_path  = r\"D:\\Portafolio oficial\\Retail Sales Trend Analysis\\data\\data\\processed\\train_with_oil_and_tx.csv\"\n",
    "chunksize    = 300_000   # ajústalo a tu memoria disponible\n",
    "\n",
    "# 3) Itero con tqdm para ver el progreso\n",
    "first_chunk = True\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(input_path, parse_dates=[\"date\"], chunksize=chunksize),\n",
    "    desc=\"Procesando chunks\",\n",
    "    unit=\"chunk\"\n",
    "):\n",
    "    # Si store_nbr viene como float, conviértelo a int para emparejar con transactions_df\n",
    "    chunk[\"store_nbr\"] = chunk[\"store_nbr\"].astype(transactions_df[\"store_nbr\"].dtype)\n",
    "    \n",
    "    # Merge con precio del crudo\n",
    "    merged = chunk.merge(oil_df, on=\"date\", how=\"left\")\n",
    "    # Merge con transacciones por fecha y tienda\n",
    "    merged = merged.merge(\n",
    "        transactions_df,\n",
    "        on=[\"date\", \"store_nbr\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Si hay fechas/tiendas sin transacciones, puedes rellenar con 0:\n",
    "    merged[\"transactions\"] = merged[\"transactions\"].fillna(0).astype(int)\n",
    "    \n",
    "    # Escribo al CSV final, añadiendo cabecera solo la primera vez\n",
    "    merged.to_csv(\n",
    "        output_path,\n",
    "        mode=\"w\" if first_chunk else \"a\",\n",
    "        header=first_chunk,\n",
    "        index=False\n",
    "    )\n",
    "    first_chunk = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
